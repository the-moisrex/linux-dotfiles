#!/bin/bash

# Help function to display usage information
print_help() {
  echo "Usage: $0 [OPTIONS] [BASE_URL]"
  echo "Finds and prints URLs from stdin, including those in HTML <a> tags, <link> tags, and <img> tags."
  echo ""
  echo "Options:"
  echo "  --base-url BASE_URL      Specify the base URL for resolving relative links."
  echo "  --verbose                Show warnings if relative URLs are found without a base URL."
  echo "  --help                   Show this help message and exit."
  echo ""
  echo "If BASE_URL is provided as a positional argument, it will be used as the base URL."
  echo "If not provided, the script will attempt to extract the base URL from the HTML."
  echo ""
  echo "Examples:"
  echo "  curl -sL http://example.com | $0 http://example.com"
  echo "  curl -sL x.com | $0 x.com"
  echo "  echo '<a href=\"/path\">link</a>' | $0 --base-url x.com"
}

# Initialize variables
base_url=""
verbose=0

# Parse command-line options
while [[ $# -gt 0 ]]; do
  case "$1" in
    --base-url)
      if [[ "$2" ]]; then
        base_url="$2"
        shift 2
      else
        echo "Error: --base-url option requires a URL." >&2
        print_help
        exit 1
      fi
      ;;
    --verbose)
      verbose=1
      shift
      ;;
    --help)
      print_help
      exit 0
      ;;
    *)
      if [[ -n "$base_url" ]]; then
        echo "Error: Cannot specify both --base-url and positional base URL." >&2
        print_help
        exit 1
      fi
      base_url="$1"
      shift
      ;;
  esac
done

# Read input from stdin
input_text=$(cat)

# Function to extract base URL from HTML
extract_base_url_from_html() {
  local text="$1"
  # Check for <base> tag
  base_tag=$(grep -o '<base[^>]*href=["'\''][^"'\''"]*["'\'']' <<< "$text" | \
             sed -e 's/.*href="\([^"]*\)".*/\1/' -e "s/.*href='\([^']*\)'.*/\1/" | head -n1)
  if [[ -n "$base_tag" ]]; then
    echo "$base_tag"
    return
  fi
  # Check for "Home" links with absolute URLs if relative URLs exist
  if grep -q 'href=["'\'']/|src=["'\'']/' <<< "$text"; then
    home_url=$(grep -o '<a[^>]*href="https\?://[^"]*"[^>]*>[[:space:]]*Home[[:space:]]*<\/a>' <<< "$text" | \
               sed 's/.*href="\([^"]*\)".*/\1/' | head -n1)
    if [[ -n "$home_url" ]]; then
      echo "$home_url" | sed 's|\(https\?://[^/]*\)/.*|\1|'
    fi
  fi
}

# Extract base URL from HTML if not provided
if [[ -z "$base_url" ]]; then
  base_url=$(extract_base_url_from_html "$input_text")
fi

# Warn if relative URLs exist but no base URL is available, only with --verbose
if [[ $verbose -eq 1 && -z "$base_url" && $(grep -q 'href=["'\'']/|src=["'\'']/' <<< "$input_text"; echo $?) -eq 0 ]]; then
  echo "Warning: Relative URLs found but no base URL provided or extracted." >&2
fi

# Preprocess base_url if provided
if [[ -n "$base_url" ]]; then
  if [[ ! "$base_url" =~ ^https?:// ]]; then
    base_url="http://$base_url"
  fi
fi

# Function to normalize paths (handles ../ and .)
normalize_path() {
  local path=$1
  local components=()
  IFS='/' read -ra parts <<< "$path"
  for part in "${parts[@]}"; do
    if [[ "$part" == "" && ${#components[@]} -eq 0 ]]; then
      components+=("")  # leading /
    elif [[ "$part" == "." || "$part" == "" ]]; then
      continue
    elif [[ "$part" == ".." ]]; then
      if [[ ${#components[@]} -gt 1 ]]; then
        unset 'components[-1]'
      fi
    else
      components+=("$part")
    fi
  done
  echo "/${components[*]:1}" | tr ' ' '/'
}

# Regex for standalone URLs
url_regex='https?://([wW]{3}\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_+.~#?&/=%]*)'

# Extract href and src attributes from <a>, <link>, and <img> tags
hrefs=$(grep -oE '<(a|link|img)[^>]* (href|src)=(["][^"]*["]|['\''][^'\'']*['\''])' <<< "$input_text" | \
        sed -e 's/.*\(href\|src\)="\([^"]*\)".*/\2/' -e "s/.*\(href\|src\)='\([^']*\)'.*/\2/")

# Extract standalone URLs
standalone_urls=$(grep -oE "$url_regex" <<< "$input_text")

# Array for all URLs
declare -a all_urls

# Process hrefs and resolve URLs
if [[ -n "$base_url" ]]; then
  # Parse base_url
  scheme=${base_url%%://*}
  host_path=${base_url#*://}
  if [[ "$host_path" == */* ]]; then
    host=${host_path%%/*}
    path=/${host_path#*/}
  else
    host=$host_path
    path="/"
  fi
  while IFS= read -r href; do
    if [[ "$href" =~ ^https?:// ]]; then
      resolved=$href
    elif [[ "$href" =~ ^// ]]; then
      resolved=${scheme}://${href#//}
    elif [[ "$href" =~ ^/ ]]; then
      resolved=${scheme}://${host}${href}
    else
      # Relative path
      if [[ "$path" == */ || "$path" == "/" ]]; then
        base_dir=$path
      else
        base_dir=${path%/*}/
        if [[ "$base_dir" == "" ]]; then
          base_dir="/"
        fi
      fi
      full_path=$base_dir$href
      resolved_path=$(normalize_path "$full_path")
      resolved=${scheme}://${host}${resolved_path}
    fi
    all_urls+=("$resolved")
  done <<< "$hrefs"
else
  # No base_url, handle URLs as is or with default protocol
  while IFS= read -r href; do
    if [[ "$href" =~ ^https?:// ]]; then
      resolved=$href
    elif [[ "$href" =~ ^// ]]; then
      resolved="http://${href#//}"
    else
      resolved=$href
    fi
    all_urls+=("$resolved")
  done <<< "$hrefs"
fi

# Add standalone URLs
mapfile -t standalone_array <<< "$standalone_urls"
all_urls=("${all_urls[@]}" "${standalone_array[@]}")

# Print all URLs, one per line
for url in "${all_urls[@]}"; do
  echo "$url"
done
